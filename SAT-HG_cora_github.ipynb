{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wi1H-Woo4tBR","outputId":"64686d38-8edf-45c9-c264-2d7f300d48cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"qhI_zCD7EUdB"},"source":["# Package Installation and Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFQmp-W2jR0Q"},"outputs":[],"source":["!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n","!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n","!pip install --upgrade numpy\n","!pip install hypernetx\n","!pip install cdlib\n","!pip install torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whwc91v55Qf6","outputId":"9f7d9119-8815-4982-9bfb-53b526893d2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: to be able to use all crisp methods, you need to install some additional packages:  {'bayanpy', 'wurlitzer', 'leidenalg', 'graph_tool', 'infomap'}\n","Note: to be able to use all crisp methods, you need to install some additional packages:  {'pyclustering', 'ASLPAw'}\n","Note: to be able to use all crisp methods, you need to install some additional packages:  {'leidenalg', 'wurlitzer', 'infomap'}\n"]}],"source":["import dgl\n","import copy\n","import bisect\n","from scipy.sparse import csr_matrix\n","import json\n","import networkx as nx\n","import networkx.algorithms.community as nx_comm\n","import torch\n","import numpy as np\n","from scipy.sparse import coo_matrix\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","from networkx.readwrite import json_graph\n","from numpy import linalg as LA\n","import random\n","from gensim.models import Word2Vec\n","import scipy\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n","from torch.nn import Parameter\n","import torch.optim as optim\n","import pdb\n","import itertools\n","from cdlib import algorithms\n","from torch_geometric.datasets import Planetoid\n","from collections import defaultdict\n","from torch_geometric.datasets import CitationFull"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxQMn06zwoWC"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"Ivha-NH9EZEh"},"source":["# Dataset Read and coms_G creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJ7aRsedEWEw"},"outputs":[],"source":["from torch_geometric.utils import to_networkx\n","path = '/content/drive/MyDrive/My Drive/Colab Notebooks/Current Project/A generalized HT/data'\n","dataset = Planetoid(path, name='cora')#Please use 'citeseer', 'pubmed' if you want to run on them\n","#dataset = CitationFull(path,name='dblp')#If you want to run on dblp dataset, just uncomment this line\n","data = dataset[0]\n","default_feat = data.x\n","label = data.y\n","numpy_array = label.numpy()\n","label = numpy_array.tolist()\n","number_class=dataset.num_classes\n","\n","# Get edge_index and edge_weight as numpy arrays\n","edge_index = data.edge_index.numpy()\n","G = nx.Graph()\n","\n","# Add edges along with default weight of 1\n","for i in range(edge_index.shape[1]):\n","    src, dst = edge_index[:, i]\n","    G.add_edge(src, dst, weight=1)\n","#convert networkx to dgl\n","g = dgl.from_networkx(G)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSq7nk4dEQ67"},"outputs":[],"source":["coms = algorithms.wCommunity(G,min_bel_degree=0.3,threshold_bel_degree=0.3)"]},{"cell_type":"markdown","metadata":{"id":"0QYRWAIn5fHF","tags":[]},"source":["# Adding global nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KM4nNTytXNlu"},"outputs":[],"source":["c=coms.communities\n","frozen_coms_G=[]\n","coms_G=[]\n","for i in range (len(c)):\n","  frozen_coms_G.append(G.subgraph(c[i]))\n","for i in range (len(frozen_coms_G)):\n","  coms_G.append(nx.Graph(frozen_coms_G[i]))\n","\n","\n","centrality = nx.closeness_centrality(G)\n","newA = set(sorted(centrality, key=centrality.get, reverse=True)[:3])\n","\n","\n","for k in range(len(coms_G)):\n","  coms_G_nodes=list( coms_G[k].nodes())\n","  for i in coms_G_nodes:\n","    for j in newA:\n","      if i!=j:\n","        coms_G[k].add_edge(i,j)\n","\n","\n","DICT = dict()\n","for i in range (len(coms_G)):\n","  DICT[i]=list(coms_G[i].nodes)"]},{"cell_type":"markdown","metadata":{"id":"6JTHdMTr3FLc"},"source":["# HyG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEj2NB-RAcy_"},"outputs":[],"source":["nl=coo_matrix((G.number_of_nodes(), G.number_of_nodes()))\n","nl.setdiag(1)\n","#nl.toarray()\n","values = nl.data\n","indices = np.vstack((nl.row, nl.col))\n","i = torch.LongTensor(indices)\n","v = torch.FloatTensor(values)\n","shape = nl.shape\n","v_feat=torch.sparse_coo_tensor(i, v, torch.Size(shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOQFS6S4Fmtn"},"outputs":[],"source":["n_nodes = G.number_of_nodes()\n","LEN=len(DICT)\n","n_hedge = LEN\n","\n","member_citing = []\n","for community in DICT.keys():\n","    members = DICT[community]\n","    for member in members :\n","        member_citing.append([member, community])\n","\n","member_community = torch.LongTensor(member_citing)\n","data_dict = {\n","        ('node', 'in', 'edge'): (member_community[:,0], member_community[:,1]),\n","        ('edge', 'con', 'node'): (member_community[:,1], member_community[:,0])\n","    }\n","\n","lst=[]\n","for i in member_citing:\n","  lst.append(i[0])\n","s=set(lst)\n","s=len(s)\n","num_nodes_dict = {'edge': LEN,'node':s}\n","hyG = dgl.heterograph(data_dict,num_nodes_dict=num_nodes_dict)\n","rows=n_nodes\n","columns=n_hedge\n","\n","\n","len_rows=rows\n","nl=np.eye(len_rows)\n","nl=torch.from_numpy(nl)\n","v_feat=nl\n","\n","len_edges=n_hedge\n","nl=np.eye(len_edges)\n","nl=torch.from_numpy(nl)\n","e_feat=nl\n","\n","\n","'''\n","e_feat=coo_matrix((columns, 16))\n","e_feat.setdiag(1)\n","#nl.toarray()\n","values = e_feat.data\n","indices = np.vstack((e_feat.row, e_feat.col))\n","i = torch.LongTensor(indices)\n","v = torch.FloatTensor(values)\n","shape = e_feat.shape\n","e_feat=torch.sparse_coo_tensor(i, v, torch.Size(shape))\n","'''\n","\n","hyG.ndata['h'] = {'edge' : e_feat.type('torch.FloatTensor'), 'node' : v_feat.type('torch.FloatTensor')}\n","e_feat = e_feat.type('torch.FloatTensor')\n","v_feat=v_feat.type('torch.FloatTensor')\n","\n","v_feat=v_feat.to(device)\n","e_feat=e_feat.to(device)\n","hyG=hyG.to(device)"]},{"cell_type":"markdown","metadata":{"id":"PVkxSReTj0Wz"},"source":["# GCN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUyzCEFaj2Gj"},"outputs":[],"source":["g=g.add_self_loop()\n","from dgl.nn import GraphConv\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, h_feats, out_feats):\n","        super(GCN, self).__init__()\n","        self.conv1 = GraphConv(in_feats, h_feats)\n","        self.conv2 = GraphConv(h_feats, out_feats)\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        return h\n","    def reset_parameters(self):\n","        self.conv1.reset_parameters()\n","        self.conv2.reset_parameters()\n","\n","\n","nl=np.eye(g.num_nodes())\n","nl=torch.from_numpy(nl)\n","nl=torch.tensor(nl,dtype=torch.long)\n","node_feat=default_feat\n","gcn_model=GCN(node_feat.shape[1],512,LEN)"]},{"cell_type":"markdown","metadata":{"id":"OvAsCjx1Gq-B"},"source":["# k-core"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snVXFhHnFci7"},"outputs":[],"source":["def k_core(graph):\n","    graph.remove_edges_from(nx.selfloop_edges(graph))\n","    k_core_dict = {node: 0 for node in graph.nodes()}\n","\n","    degrees = dict(graph.degree())\n","    if not degrees:\n","        print(\"Graph has no nodes or edges.\")\n","        return\n","    else:\n","        max_k = max(degrees.values())\n","        nodes_sorted_by_degree = sorted(degrees, key=degrees.get)\n","\n","    for k in range(1, max_k + 1):\n","        while nodes_sorted_by_degree and degrees[nodes_sorted_by_degree[0]] < k:\n","            node = nodes_sorted_by_degree.pop(0)\n","            k_core_dict[node] = degrees[node]\n","            for neighbor in graph[node]:\n","                if degrees[neighbor] > degrees[node]:\n","                    degrees[neighbor] -= 1\n","                    position = nodes_sorted_by_degree.index(neighbor)\n","                    nodes_sorted_by_degree.pop(position)\n","                    bisect.insort(nodes_sorted_by_degree, neighbor, lo=position)\n","            del degrees[node]\n","            graph.remove_node(node)\n","\n","    max_value = sum(k_core_dict.values())\n","    if max_value == 0:\n","        return k_core_dict\n","    else:\n","        normalized_dict = {key: value / max_value for key, value in k_core_dict.items()}\n","        return normalized_dict"]},{"cell_type":"markdown","metadata":{"id":"apjV-g2ATbca"},"source":["# Laplacian eigenvectors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"5-aZGRGcQZNB","outputId":"d6aa7ae7-6a71-473b-f033-03bf9b644eb4"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nL = nx.laplacian_matrix(G_B)\\nL = csr_matrix(L)  # Convert to sparse matrix for efficient computation\\nL = L.toarray()\\n'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["building_edges=[]\n","member_nodes=[]\n","for i in DICT.keys():\n","  for j in DICT[i]:\n","    j=str(j)+'a'\n","    building_edges.append((j,i))\n","    member_nodes.append(j)\n","member_nodes=set(member_nodes)\n","\n","\n","B = nx.Graph()\n","B.add_edges_from(building_edges)\n","\n","node=member_nodes\n","from networkx.algorithms import bipartite\n","G_B = bipartite.weighted_projected_graph(B, node)\n","\n","\n","A=nx.to_numpy_array(G_B)\n","degree=[]\n","for i in range (len(A)):\n","  c=0\n","  for j in range (len(A)):\n","    c=c+A[i][j]\n","  degree.append(c)\n","degree=np.array(degree)\n","D=np.diag(degree)\n","L=D-A\n","\n","'''\n","import scipy.sparse as sp\n","A = sp.csr_matrix(nx.adjacency_matrix(G_B))\n","degree = A.sum(axis=1).A1\n","D = sp.diags(degree)\n","L = D - A\n","'''\n","'''\n","L = nx.laplacian_matrix(G_B)\n","L = csr_matrix(L)  # Convert to sparse matrix for efficient computation\n","L = L.toarray()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqTwru5CTZEg"},"outputs":[],"source":["# Compute eigenvectors and eigenvalues\n","eign_v, eign_vec = LA.eigh(L)\n","\n","# Perform random sign flipping\n","for i in range(eign_vec.shape[1]):\n","    maximum_absolute_value_index = np.argmax(np.abs(eign_vec[:, i]))\n","    sign = np.sign(eign_vec[maximum_absolute_value_index, i])\n","    random_signs = np.random.choice([-1, 1], size=eign_vec.shape[0])\n","    eign_vec[:, i] *= sign * random_signs\n","\n","# Convert eigenvectors and eigenvalues to torch tensors\n","eign_vec = torch.tensor(eign_vec)\n","eign_vec = eign_vec.type(torch.FloatTensor).to(device)\n","\n","eign_v = torch.tensor(eign_v)\n","eign_v = eign_v.type(torch.FloatTensor).to(device)"]},{"cell_type":"markdown","metadata":{"id":"nj5rrmffIT-C"},"source":["# HyGNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgPhABYjPVay"},"outputs":[],"source":["centrality_values=list(nx.closeness_centrality(G).values())\n","centrality_values = torch.LongTensor(centrality_values).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8aoYu6GEU4q"},"outputs":[],"source":["# Find the number of keys each value appears in\n","value_counts = defaultdict(int)\n","for key, values in DICT.items():\n","    unique_values = set(values)\n","    for value in unique_values:\n","        value_counts[value] += 1\n","\n","# Calculate the score for each value\n","total_keys = len(DICT)\n","value_scores = {}\n","uniqueness_list=[]\n","for value, count in value_counts.items():\n","    score = 1 - (count / total_keys)\n","    value_scores[value] = score\n","    uniqueness_list.append(score)\n","\n","uniqueness = np.array(uniqueness_list)\n","uniqueness = torch.LongTensor(uniqueness).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yc8a5XTEmjPp"},"outputs":[],"source":["def local_clustering_coefficient(com_graph):\n","    clustering_coefficients = {}\n","    hyperedge_nodes = list(com_graph.nodes())\n","    for node in hyperedge_nodes:\n","        neighbors_in_hyperedge = set(com_graph.neighbors(node))\n","        n = len(neighbors_in_hyperedge)\n","        if n > 1:\n","            # Calculate the number of existing connections between neighbors\n","            existing_connections = sum(1 for u, v in itertools.combinations(neighbors_in_hyperedge, 2) if com_graph.has_edge(u, v))\n","            total_possible_connections = n * (n - 1) // 2\n","            clustering_coefficients[node] = existing_connections / total_possible_connections\n","        else:\n","            clustering_coefficients[node] = 0\n","\n","    return clustering_coefficients\n","\n","\n","def hyperedge_clustering_coefficient(com_graph):\n","    hyperedge_nodes = list(com_graph.nodes())\n","    triangle_count = 0\n","\n","    for idx, u in enumerate(hyperedge_nodes):\n","        for v in hyperedge_nodes[idx + 1:]:\n","            if com_graph.has_edge(u, v):  # Ensure there is an edge between u and v\n","                # common_neighbors = list(nx.common_neighbors(com_graph, u, v))\n","                # for w in common_neighbors:\n","                #     if com_graph.has_edge(u, w) and com_graph.has_edge(v, w):  # Check if u, v, w form a triangle\n","                        triangle_count += len(list(nx.common_neighbors(com_graph, u, v)))\n","                        # triangle_count += 1\n","\n","    # Since each triangle is counted three times, divide by 3\n","    triangle_count //= 3\n","    n = len(hyperedge_nodes)\n","    total_possible_triangles = n * (n - 1) * (n - 2) // 6\n","\n","    clustering_coefficient = triangle_count / total_possible_triangles if total_possible_triangles > 0 else 0.0\n","    return clustering_coefficient\n","\n","size_of_coms_G=len(coms_G)\n","dict_hyperedge_clustering_coefficient={}\n","for i in range (size_of_coms_G):\n","  dict_hyperedge_clustering_coefficient[i]=hyperedge_clustering_coefficient(coms_G[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVpOXoJmJJCK"},"outputs":[],"source":["num_of_edge=int(hyG.num_edges()/2)\n","\n","class SAT_HG(nn.Module):\n","    # edge attention  version\n","    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, num_class, dropout):\n","        super(SAT_HG, self).__init__()\n","\n","        self.query_dim = query_dim\n","        self.vtx_lin_1layer = torch.nn.Linear(input_dim, vertex_dim)\n","        self.vtx_lin = torch.nn.Linear(vertex_dim, vertex_dim)\n","\n","        self.qe_lin = torch.nn.Linear(edge_dim, query_dim)\n","        self.kv_lin = torch.nn.Linear(vertex_dim, query_dim)\n","        self.vv_lin = torch.nn.Linear(vertex_dim, edge_dim)\n","\n","        self.qv_lin = torch.nn.Linear(vertex_dim, query_dim)\n","        self.ke_lin = torch.nn.Linear(edge_dim, query_dim)\n","        self.ve_lin = torch.nn.Linear(edge_dim, vertex_dim)\n","\n","\n","        self.cls = nn.Linear(vertex_dim, num_class)\n","        self.cs_embedding = nn.Embedding(num_embeddings=len(centrality_values), embedding_dim=LEN)\n","        self.un_embedding = nn.Embedding(num_embeddings=len(uniqueness), embedding_dim=LEN)\n","\n","        self.eign_vec_lin=torch.nn.Linear(eign_vec.shape[1],vertex_dim)\n","        self.edge_lin_1layer=torch.nn.Linear(e_feat.shape[0],e_feat.shape[1])\n","\n","\n","        #add&norm\n","        self.layer_norm1 = nn.LayerNorm(vertex_dim)\n","        self.layer_norm2 = nn.LayerNorm(vertex_dim)\n","\n","        #ffn\n","        self.linear1 = nn.Linear(vertex_dim, query_dim)\n","        self.linear3 = nn.Linear(vertex_dim, query_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(query_dim, vertex_dim)\n","        self.linear4 = nn.Linear(query_dim, vertex_dim)\n","\n","\n","    def add_and_norm1(self, x, residual):\n","        output = x + residual\n","        output = self.layer_norm1(output)\n","        return output\n","\n","    def ffn1(self, x):\n","        output = self.linear1(x)\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","        output = self.linear2(output)\n","        return output\n","\n","    def add_and_norm2(self, x, residual):\n","        output = x + residual\n","        output = self.layer_norm2(output)\n","        return output\n","\n","    def ffn2(self, x):\n","        output = self.linear3(x)\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","        output = self.linear4(output)\n","        return output\n","\n","    def attention(self, edges):\n","        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n","        c=attn_score/np.sqrt(self.query_dim)\n","        return {'Attn': c}\n","\n","    def message_func(self, edges):\n","        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n","\n","    def reduce_func1(self, nodes):\n","        #attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n","        #aggr = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n","\n","\n","        lst_node=nodes.nodes().tolist()\n","        updated_attention_score_list=[]\n","        node_embedding_gcn_list=[]\n","        for i in range (len(lst_node)):\n","          node=lst_node[i]\n","          attn_score=nodes.mailbox['Attn'][i]\n","\n","          # Calculate clustering coefficients for nodes within the hyperedge\n","          com_graph=nx.Graph()\n","          com_graph=copy.deepcopy(coms_G[node])\n","\n","          clustering_coefficients = local_clustering_coefficient(coms_G[node])\n","          clustering_coefficients=torch.Tensor(list(clustering_coefficients.values())).to(device)\n","\n","          k_core_values=list(k_core(com_graph).values())\n","          k_core_values=torch.Tensor(k_core_values).to(device)\n","\n","          combined_local_structural_values=k_core_values+clustering_coefficients\n","\n","          updated_attention_score=torch.add(attn_score,combined_local_structural_values)\n","          updated_attention_score=updated_attention_score.tolist()\n","          updated_attention_score_list.append(updated_attention_score)\n","\n","        updated_attention_score_list=torch.Tensor(updated_attention_score_list).to(device)\n","\n","\n","        updated_attention_score_list = F.softmax(updated_attention_score_list, dim=1)\n","        aggr = torch.sum(updated_attention_score_list.unsqueeze(-1) *nodes.mailbox['v'], dim=1)\n","\n","        return {'h': aggr}\n","\n","    def reduce_func2(self, nodes):\n","        lst_node=nodes.nodes().tolist()\n","\n","        #attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n","        #aggr = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n","\n","        updated_attention_score_list=[]\n","        for i in range (len(lst_node)):\n","          node=lst_node[i]\n","          attn_score=nodes.mailbox['Attn'][i]\n","\n","          key_=[key for key,value in DICT.items() if node in value]\n","          combined_global_structural_values=[]\n","\n","          for k in key_:\n","\n","            #c=hyperedge_clustering_coefficient(coms_G[k])\n","            c=dict_hyperedge_clustering_coefficient[k]\n","            d=c+(coms_G[k].number_of_nodes()/G.number_of_nodes())\n","            combined_global_structural_values.append(d)\n","\n","          combined_global_structural_values=torch.Tensor(combined_global_structural_values).to(device)\n","          updated_attention_score=torch.add(attn_score,combined_global_structural_values)\n","          updated_attention_score=updated_attention_score.tolist()\n","          updated_attention_score_list.append(updated_attention_score)\n","\n","        updated_attention_score_list=torch.Tensor(updated_attention_score_list).to(device)\n","        updated_attention_score_list = F.softmax(updated_attention_score_list, dim=1)\n","\n","        aggr = torch.sum(updated_attention_score_list.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n","\n","\n","        return {'h': aggr}\n","\n","    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n","\n","        with hyG.local_scope():\n","\n","            if first_layer:\n","                feat_v = self.vtx_lin_1layer(vfeat)\n","                pe=self.eign_vec_lin(eign_vec)\n","                cs=self.cs_embedding(centrality_values)\n","                un=self.un_embedding(uniqueness)\n","\n","\n","                feat_v=feat_v+pe\n","                feat_v_gcn=gcn_model(g,node_feat).to(device)\n","                feat_v=feat_v+feat_v_gcn\n","                feat_v=feat_v+cs\n","                feat_v=feat_v+un\n","            else:\n","                feat_v = self.vtx_lin(vfeat)\n","                pe=self.eign_vec_lin(eign_vec)\n","                cs=self.cs_embedding(centrality_values)\n","                un=self.un_embedding(uniqueness)\n","\n","\n","                feat_v=feat_v+pe\n","                feat_v_gcn=gcn_model(g,node_feat).to(device)\n","                feat_v=feat_v+feat_v_gcn\n","                feat_v=feat_v+cs\n","                feat_v=feat_v+un\n","\n","            feat_e = efeat\n","\n","            # node attention\n","            hyG.ndata['h'] = {'node': feat_v}\n","            hyG.ndata['k'] = {'node' : self.kv_lin(feat_v)}\n","            hyG.ndata['v'] = {'node' : self.vv_lin(feat_v)}\n","            hyG.ndata['q'] = {'edge' : self.qe_lin(feat_e)}\n","            hyG.apply_edges(self.attention, etype='in')\n","            hyG.update_all(self.message_func, self.reduce_func1, etype='in')\n","            feat_e_transformed = hyG.ndata['h']['edge']\n","\n","            feat_e_add_norm=self.add_and_norm1(feat_e_transformed,feat_e)\n","            feat_e_ffn=self.ffn1(feat_e_add_norm)\n","            feat_e=self.add_and_norm1(feat_e_ffn,feat_e_add_norm)\n","\n","            # edge attention\n","            hyG.ndata['k'] = {'edge' : self.ke_lin(feat_e)}\n","            hyG.ndata['v'] = {'edge' : self.ve_lin(feat_e)}\n","            hyG.ndata['q'] = {'node' : self.qv_lin(feat_v)}\n","            hyG.apply_edges(self.attention, etype='con')\n","            hyG.update_all(self.message_func, self.reduce_func2, etype='con')\n","            feat_v_transformed = hyG.ndata['h']['node']\n","\n","\n","            feat_v_add_norm=self.add_and_norm2(feat_v_transformed,feat_v)\n","            feat_v_ffn=self.ffn2(feat_v_add_norm)\n","            feat_v=self.add_and_norm2(feat_v_ffn,feat_v_add_norm)\n","\n","            if not last_layer :\n","                feat_v = F.dropout(feat_v, self.dropout)\n","                return feat_v\n","            if last_layer:\n","                feat_v = self.dropout(feat_v)\n","                pred=self.cls(feat_v)\n","                #return feat_v, feat_e, pred\n","                return pred\n","            else:\n","                return [g, feat_v, feat_e]\n"]},{"cell_type":"markdown","metadata":{"id":"-Nb41k0xRAS2"},"source":["# Train-test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bvmau8ESMt3F"},"outputs":[],"source":["'''\n","Don't run second time\n","'''\n","with open('/content/drive/MyDrive/My Drive/Colab Notebooks/Current Project/A generalized HT/data/label_cora.txt', 'w') as f:\n","    for item in label:\n","        f.write(\"%s \\n\" % item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4C3mphYMvEt"},"outputs":[],"source":["label_data=pd.read_csv('/content/drive/MyDrive/My Drive/Colab Notebooks/Current Project/A generalized HT/data/label_cora.txt',header=None)\n","label=label_data[0].tolist()\n","\n","test_size=0.25\n","\n","train_label, test_label= train_test_split(label,test_size=test_size, random_state=7)\n","train_label=torch.tensor(train_label,dtype=torch.long).to(device)\n","size=int(len(label)*0.25)\n","val_label=train_label[0:size]\n","train_label=train_label[size:]\n","test_label=torch.tensor(test_label,dtype=torch.long).to(device)"]},{"cell_type":"markdown","metadata":{"id":"BkQh-TouHGgE"},"source":["# Multi-Head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGL0ydfGuejD"},"outputs":[],"source":["class Multi_head_attn(nn.Module):\n","\n","    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, number_class, dropout,num_heads,merge='cat'):\n","        super(Multi_head_attn, self).__init__()\n","\n","        self.heads = nn.ModuleList()\n","        for i in range(num_heads):\n","            self.heads.append(SAT_HG(input_dim, query_dim, vertex_dim, edge_dim, number_class, dropout ))\n","        self.merge = merge\n","\n","    def forward(self, hyG, v_feat,e_feat,first_layer,second_layer):\n","        head_outs = [attn_head(hyG,v_feat,e_feat,first_layer,second_layer) for attn_head in self.heads]\n","\n","        if self.merge == 'cat':\n","            # concat on the output feature dimension (dim=1)\n","            return torch.cat(head_outs, dim=1)\n","        else:\n","            # merge using average\n","            return torch.mean(torch.stack(head_outs))\n","    def reset_parameters(self):\n","        self.heads.reset_parameters()\n","class SAT_HG_attn(nn.Module):\n","    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, number_class, dropout, num_heads):\n","        super(SAT_HG_attn, self).__init__()\n","        self.layer1 = Multi_head_attn(input_dim, query_dim, vertex_dim, edge_dim, number_class, dropout,num_heads )\n","    def forward(self,hyG,v_feat,e_feat,first_layer,second_layer):\n","        x = self.layer1(hyG, v_feat,e_feat,True,True)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FW-vQJhUuZmy"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzYks5SxM1He","outputId":"9001825a-c3b0-4184-ac36-d405c419531d"},"outputs":[{"name":"stdout","output_type":"stream","text":["In epoch 0, train loss: 3.4929, val_acc: 0.0857 (best_val_acc: 0.0857)\n","In epoch 10, train loss: 0.1207, val_acc: 0.9158 (best_val_acc: 0.9173)\n","In epoch 20, train loss: 0.0340, val_acc: 0.8833 (best_val_acc: 0.9173)\n","In epoch 30, train loss: 0.0131, val_acc: 0.8804 (best_val_acc: 0.9173)\n","In epoch 40, train loss: 0.0027, val_acc: 0.8759 (best_val_acc: 0.9173)\n","In epoch 50, train loss: 0.0009, val_acc: 0.8744 (best_val_acc: 0.9173)\n","In epoch 60, train loss: 0.0007, val_acc: 0.8700 (best_val_acc: 0.9173)\n","In epoch 70, train loss: 0.0005, val_acc: 0.8833 (best_val_acc: 0.9173)\n","In epoch 80, train loss: 0.0001, val_acc: 0.8744 (best_val_acc: 0.9173)\n","In epoch 90, train loss: 0.0002, val_acc: 0.8789 (best_val_acc: 0.9173)\n","In epoch 100, train loss: 0.0001, val_acc: 0.8774 (best_val_acc: 0.9173)\n"]}],"source":["model =SAT_HG_attn(v_feat.shape[1], 64, LEN, LEN,  number_class, 0.5,4)\n","model=model.to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","optimizer = torch.optim.Adam(itertools.chain(model.parameters(), gcn_model.parameters()), lr=0.01)\n","best_val_acc = 0\n","patience=0\n","\n","for i in range(500):\n","    model.train()\n","    pred= model(hyG, v_feat, e_feat, True,True)\n","    train_pred, test_pred= train_test_split( pred,test_size=test_size, random_state=7)\n","    val_pred=train_pred[0:size]\n","    train_pred=train_pred[size:]\n","\n","\n","    loss = loss_fn(train_pred, train_label)\n","    pred_cls = torch.argmax(train_pred, -1)\n","    train_acc = torch.eq(pred_cls, train_label).sum().item()/len(train_label)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    with torch.no_grad():\n","\n","      model.eval()\n","      val_cls = torch.argmax(val_pred, -1)\n","      val_acc = torch.eq(val_cls, val_label).sum().item()/len(val_label)\n","\n","      # Save the best validation accuracy and the corresponding test accuracy.\n","      if best_val_acc < val_acc:\n","        best_val_acc = val_acc\n","        E=i\n","        patience=0\n","        torch.save(test_pred,'latest.pth')\n","      else:\n","        patience+=1\n","      if patience==100:\n","        break\n","    if i % 10 == 0:\n","      print('In epoch {}, train loss: {:.4f}, val_acc: {:.4f} (best_val_acc: {:.4f})'.format(i, loss, val_acc, best_val_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBYY0KTSM2f6","outputId":"8bd5efb1-c437-4b94-eba9-8f35289fda6f"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Best Epoch: 5, Accuracy: 0.9306, Precision: 0.9309, Recall: 0.9306, F1-score 0.9302\n"]}],"source":["best_test_perd=torch.load('latest.pth')\n","with torch.no_grad():\n","  test_pred=best_test_perd\n","  test_cls = torch.argmax(test_pred, -1)\n","  test_acc = torch.eq(test_cls, test_label).sum().item()/len(test_label)\n","  print(' Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}'.format(E,accuracy_score(test_label.cpu(),test_cls.cpu()),precision_score(test_label.cpu(),test_cls.cpu(),average='weighted'),recall_score(test_label.cpu(),test_cls.cpu(),average='weighted'),f1_score(test_label.cpu(),test_cls.cpu(),average='weighted')))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
